{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TidyMachineLearning.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgm1ZRXQhb0ns7lErb62DQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NanaAkwasiAbayieBoateng/TidyMachineLearning/blob/master/TidyMachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfkxDTIkbEXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# activate R magic\n",
        "import rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpLSDWDObVn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%capture  #hide cell output\n",
        "%%R\n",
        "install.packages(\"pacman\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrF44hLuh0vK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "pacman::p_load(tidyverse,rio,tidymodels)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UDUVztsbiL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%%R\n",
        "\n",
        "pacman::p_load(skimr,kableExtra,caret,recipes,rsample,yardstick,pROC,rio,\n",
        "               xgboost,mlr,Hmisc,MLmetrics,data.table,furrr)\n",
        "    \n",
        "    \n",
        "#Load  variable importance plot\n",
        "source(\"https://raw.githubusercontent.com/NanaAkwasiAbayieBoateng/MLTools/master/Varplot.R\")\n",
        "source(\"https://raw.githubusercontent.com/NanaAkwasiAbayieBoateng/MLTools/master/EvaluationMetrics.R\")\n",
        "\n",
        "\n",
        "theme_set(theme_pubclean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRO9OpK5fEuj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#### XGBoost Grid  Hyperparameter Optimization\n",
        "\n",
        "Load the data, included in the {mlrbench} package. The original data has the target variable cmedv is a continuous variable. It is converted to a binary variable here based on below and above its median value. This was done because we are interested in training a classification model. The data is included in the mlbench library.\n",
        "\n",
        "#### Description of Data\n",
        "The data is available on kaggle. The link to is [here](https://www.kaggle.com/patelprashant/employee-attrition)\n",
        "\n",
        "- Context\n",
        "The key to success in any organization is attracting and retaining top talent. I’m an HR analyst at my company, and one of my tasks is to determine which factors keep employees at my company and which prompt others to leave. I need to know what factors I can change to prevent the loss of good people. Watson Analytics is going to help.\n",
        "\n",
        "- Content\n",
        "I have data about past and current employees in a spreadsheet on my desk top. It has various data points on our employees, but I’m most interested in whether they’re still with my company or whether they’ve gone to work somewhere else. And I want to understand how this relates to workforce attrition.\n",
        "\n",
        "- Education\n",
        "1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'\n",
        "\n",
        "- EnvironmentSatisfaction\n",
        "1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n",
        "\n",
        "- JobInvolvement\n",
        "1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n",
        "\n",
        "- JobSatisfaction\n",
        "1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n",
        "\n",
        "- PerformanceRating\n",
        "1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'\n",
        "\n",
        "-- RelationshipSatisfaction\n",
        "1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n",
        "\n",
        "- WorkLifeBalance\n",
        "1 'Bad' 2 'Good' 3 'Better' 4 'Best'\n",
        "\n",
        "Acknowledgements\n",
        "https://www.ibm.com/communities/analytics/watson-analytics-blog/watson-analytics-use-case-for-hr-retaining-valuable-employees/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NQkkU_ie9cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "\n",
        "path = \"https://raw.githubusercontent.com/NanaAkwasiAbayieBoateng/Datasets/master/EmployeeAttrition.csv\"\n",
        "\n",
        "employee_data  = rio::import(path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aZHKefXj_gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "install.packages(\"gtsummary\")\n",
        "install.packages(\"gt\")\n",
        "install.packages(\"kable\")\n",
        "install.packages(\"kableExtra\")\n",
        "#install.packages(\"glue\")\n",
        "devtools::install_github(\"tidyverse/glue\")\n",
        "install.packages(\"mlr\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvn9ytTQm4yA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "#library(gtsummary)\n",
        "#library(gt)\n",
        "# make dataset with a few variables to summarize\n",
        "#trial2 <- trial %>% dplyr::select(trt, age, grade, response)\n",
        "\n",
        "# summarize the data with our package\n",
        "#table1 <- tbl_summary(employee_data)\n",
        "\n",
        "#tbl_summary(employee_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TFxosOcfPgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "pacman::p_load(kable,kableExtra)\n",
        "\n",
        "\n",
        "# Perform a quick summary with skim()\n",
        "#employee_data %>%  skimr::skim() \n",
        "\n",
        "\n",
        "skimr::skim(employee_data)%>%\n",
        "  kable() %>%\n",
        "  kable_styling()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSDXTwLgpjqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        " mlr::summarizeColumns(employee_data)%>%  \n",
        "  \n",
        "  kable(escape = F, align = \"c\") %>%\n",
        "  kable_styling(c(\"striped\", \"condensed\"), full_width = F)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rwKspAFfV_2",
        "colab_type": "text"
      },
      "source": [
        "#### DATA SPLITTING \n",
        "The dataset would be split into traning and test sets using the rsample package. The training set is used to train the model and test set would be used for model validation and performance measures.\n",
        "\n",
        "The rsample package is used to  create an object that contains the information on how to split the data, and then two more rsample functions to create data frames for the training and testing sets: measure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZGRBoOcfPdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "train_test_split <- rsample::initial_split(boston3,strata = \"price\", prop = 0.7)\n",
        "housing_train <- training(train_test_split)\n",
        "housing_test <- testing(train_test_split)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddAVcDuAfPZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "validation_data <- mc_cv(housing_train, prop = 0.9, times = 30)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ysx-NSCfla6",
        "colab_type": "text"
      },
      "source": [
        "#### CREATE RECIPE AND ROLES ︎\n",
        "To get started, let’s create a recipe for an xgboost  model. Before training the model, we can use a recipe to create a few new predictors and conduct some preprocessing required by the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioZJ2ftLfPWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "prep_recipe <- function(dataset){\n",
        "    recipe(price ~ ., data = dataset) %>%\n",
        "     #   step_center(all_numeric()) %>%\n",
        "     #   step_scale(all_numeric()) %>%\n",
        "        step_dummy(all_nominal(),-all_outcomes())%>%\n",
        "    prep(data = dataset,retain = TRUE )\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "recipe_prepped <- prep_recipe(dataset = housing_train)\n",
        "\n",
        "train_data <- bake(recipe_prepped, new_data = housing_train)\n",
        "test_data  <- bake(recipe_prepped, new_data = housing_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTzXOmdWfPSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "\n",
        "my_xgb <- function(mtry, trees,min_n,tree_depth,learn_rate, \n",
        "                   loss_reduction,sample_size, split, id){\n",
        "    analysis_set <- analysis(split)\n",
        "     #analysis_prep <- prep(simple_recipe(analysis_set), training = analysis_set)\n",
        "    analysis_prep <- prep_recipe(analysis_set)\n",
        "    analysis_processed <- bake(analysis_prep, new_data = analysis_set)\n",
        "    #parsnip::xgb_train()\n",
        "   model <- boost_tree(mode = \"classification\", mtry = mtry, trees = trees,\n",
        "  min_n = min_n, tree_depth = tree_depth, learn_rate = learn_rate,\n",
        "  loss_reduction = loss_reduction, sample_size = sample_size)%>%\n",
        "      set_engine(\"xgboost\") %>%\n",
        "        fit(price ~ ., data = analysis_processed)\n",
        "    #model <- rand_forest(mode = \"regression\",mtry = mtry, trees = trees) %>%\n",
        "    #    set_engine(\"ranger\", importance = 'impurity') %>%\n",
        "     #   fit(price ~ ., data = analysis_processed)\n",
        "    assessment_set <- assessment(split)\n",
        "    assessment_prep <- prep(prep_recipe(assessment_set), testing = assessment_set)\n",
        "    assessment_processed <- bake(assessment_prep, new_data = assessment_set)\n",
        "    tibble::tibble(\"id\" = id,\n",
        "        \"truth\" = assessment_processed$price,\n",
        "        \"prediction\" = unlist(predict(model, new_data = assessment_processed)))\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "my_xgb_reg <- function(mtry, trees,min_n,tree_depth,learn_rate, \n",
        "                   loss_reduction,sample_size, split, id){\n",
        "     analysis_set <- analysis(split)\n",
        "    #analysis_prep <- prep(simple_recipe(analysis_set), training = analysis_set)\n",
        "    analysis_prep <- prep_recipe(analysis_set)\n",
        "    analysis_processed <- bake(analysis_prep, new_data = analysis_set)\n",
        "    #parsnip::xgb_train()\n",
        "   model <- boost_tree(mode = \"regression\", mtry = mtry, trees = trees,\n",
        "  min_n = min_n, tree_depth = tree_depth, learn_rate = learn_rate,\n",
        "  loss_reduction = loss_reduction, sample_size = sample_size)%>%\n",
        "      set_engine(\"xgboost\") %>%\n",
        "        fit(price ~ ., data = analysis_processed)\n",
        "     #model <- rand_forest(mode = \"regression\",mtry = mtry, trees = trees) %>%\n",
        "    #    set_engine(\"ranger\", importance = 'impurity') %>%\n",
        "     #   fit(price ~ ., data = analysis_processed)\n",
        " assessment_set <- assessment(split)\n",
        "assessment_prep <- prep(prep_recipe(assessment_set), testing = assessment_set)\n",
        "  assessment_processed <- bake(assessment_prep, new_data = assessment_set)\n",
        "tibble::tibble(\"id\" = id,\n",
        "        \"truth\" = assessment_processed$price,\n",
        "        \"prediction\" = unlist(predict(model, new_data = assessment_processed)))\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__M_-wOBf3q-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "subsample\t- sample_size   proprtion\n",
        "trees  _\n",
        "tree_depth - max_depth   integer\n",
        "learn_rate   -eta    proportion \n",
        "mtry   -colsample_bytree\n",
        "\n",
        "los_reduction  -  gamma , proportion\n",
        "min_n  -  min_child_weight, proprtion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ifX_G8jf-JM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "xgb_tuning <- function(param){\n",
        "    validation_data <-  validation_data\n",
        "    mtry <- param[1]\n",
        "    trees <- param[2]\n",
        "    min_n <- param[3]\n",
        "    tree_depth <- param[4]\n",
        "    learn_rate <- param[5]\n",
        "    loss_reduction <- param[6]\n",
        "    sample_size  <- param[7]\n",
        "\n",
        "    results <- purrr::map2_df(.x = validation_data$splits,\n",
        "                       .y = validation_data$id,\n",
        "                       ~my_xgb(mtry = mtry, trees = trees, \n",
        "                               min_n=min_n,tree_depth=tree_depth,\n",
        "                               learn_rate=learn_rate, \n",
        "                       loss_reduction=loss_reduction,\n",
        "                       sample_size=sample_size,\n",
        "                               \n",
        "                               split = .x, id = .y))\n",
        "\n",
        "    results %>%\n",
        "        group_by(id) %>%\n",
        "        f_meas(truth, prediction) %>%\n",
        "        summarise(mean_f_meas = mean(.estimate)) %>%\n",
        "        pull\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "xgb_reg_tuning <- function(param){\n",
        "    validation_data <-  validation_data\n",
        "    mtry <- param[1]\n",
        "    trees <- param[2]\n",
        "    min_n <- param[3]\n",
        "    tree_depth <- param[4]\n",
        "    learn_rate <- param[5]\n",
        "    loss_reduction <- param[6]\n",
        "    sample_size  <- param[7]\n",
        "\n",
        "    results <- purrr::map2_df(.x = validation_data$splits,\n",
        "                       .y = validation_data$id,\n",
        "                       ~my_xgb_reg(mtry = mtry, trees = trees, \n",
        "                               min_n=min_n,tree_depth=tree_depth,\n",
        "                               learn_rate=learn_rate, \n",
        "                       loss_reduction=loss_reduction,\n",
        "                       sample_size=sample_size,\n",
        "                               \n",
        "                               split = .x, id = .y))\n",
        "\n",
        "    results %>%\n",
        "        group_by(id) %>%\n",
        "      yardstick::rmse(truth, prediction) %>%\n",
        "        summarise(mean_rmse = mean(.estimate)) %>%\n",
        "        pull\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LSglPskgJmJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1l49wMkgLDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "results_example <- map2_df(.x = validation_data$splits,\n",
        "                           .y = validation_data$id,\n",
        "                           ~my_xgb( mtry = 3, trees = 200,\n",
        "                                    min_n=0.2,tree_depth=7,learn_rate=0.01,\n",
        "                                   loss_reduction=0.01,sample_size=0.7,  \n",
        "                                   split = .x, id = .y))\n",
        "head(results_example)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-g1RTTEgQio",
        "colab_type": "text"
      },
      "source": [
        "I can now compute the RMSE when mtry = 3 and trees = 200:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MlaxQHHgRe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "\n",
        "Accuracy=results_example %>%\n",
        "    group_by(id) %>%\n",
        "    accuracy(truth, prediction) %>%\n",
        "    summarise(mean_acc = mean(.estimate)) %>%\n",
        "    pull\n",
        "\n",
        "\n",
        "\n",
        "Precision = results_example %>%\n",
        "    group_by(id) %>%\n",
        "    precision(truth, prediction) %>%\n",
        "    summarise(mean_prec = mean(.estimate)) %>%\n",
        "    pull\n",
        "\n",
        "\n",
        "Recall = results_example %>%\n",
        "    group_by(id) %>%\n",
        "    recall(truth, prediction) %>%\n",
        "    summarise(mean_recall = mean(.estimate)) %>%\n",
        "    pull\n",
        "\n",
        "F_mea = results_example %>%\n",
        "    group_by(id) %>%\n",
        "    f_meas(truth, prediction) %>%\n",
        "    summarise(mean_f = mean(.estimate)) %>%\n",
        "    pull\n",
        "\n",
        "\n",
        "Balanced_Accuracy = results_example %>%\n",
        "    group_by(id) %>%\n",
        "    bal_accuracy(truth, prediction) %>%\n",
        "    summarise(mean_bal_acc = mean(.estimate)) %>%\n",
        "    pull\n",
        "\n",
        "\n",
        "# AUC= results_example %>%\n",
        "#   #mutate(truth= as.double(truth)-1)%>%\n",
        "#     group_by(id) %>%\n",
        "#     roc_auc(truth, prediction) %>%\n",
        "#     summarise(mean_bal_acc = mean(.estimate)) %>%\n",
        "#     pull\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "library(glue)\n",
        "\n",
        "#glue('AUC is {round(AUC,3)}.')\n",
        "glue('F Meaure  is {round(F_mea,3)}.')\n",
        "glue('Balanced_Accuracy is {round(Balanced_Accuracy,3)}.')\n",
        "glue('Recall is {round(Recall,3)}.')\n",
        "glue('Precision is {round(Precision,3)}.')\n",
        "glue('Accuracy is {round(Accuracy,3)}.')\n",
        "\n",
        "\n",
        "# predictions_glm %>%\n",
        "#   conf_mat(price, .pred_class) %>%\n",
        "#   pluck(1) %>%\n",
        "#   as_tibble() %>%\n",
        "#   ggplot(aes(Prediction, Truth, alpha = n)) +\n",
        "#   geom_tile(show.legend = FALSE) +\n",
        "#   geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n",
        "# \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je-Jk8gfgc0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "cross_val_tbl <- vfold_cv(train_data, v = 10)\n",
        "cross_val_tbl\n",
        "\n",
        "cross_val_tbl$splits %>%\n",
        "  pluck(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vFQX5u8gik1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Grid Search Hyperparameter Tuning\n",
        "\n",
        "Using Grid_data,the number of models to be trained 4*3*3*3 equal 108 different models. These comibations will take far too much time to run. For this reason we would limit ourselves to a smaller combination of hyperparameter values using grid_data.\n",
        "\n",
        "mtry <- param[1]\n",
        "    trees <- param[2]\n",
        "    min_n <- param[3]\n",
        "    tree_depth <- param[4]\n",
        "    learn_rate <- param[5]\n",
        "    loss_reduction <- param[6]\n",
        "    sample_size  <- param[7]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x52GkIx0glLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "\n",
        "\n",
        "Grid_data= expand.grid(mtry=4:7,trees=c(200,250,300),\n",
        "                       learn_rate=c(0.001,0.01,0.1),tree_depth=c(6,7,8))\n",
        "\n",
        "#metric_results=furrr::future_map2_dbl(.x=4:5,.y=200:201,~xgb_tuning(c(.x, .y,0.5,10,0.01,0.2,0.8)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "grid_data= tibble(mtry=3:10,trees=c(50,100,150,200,250,300,350,400))\n",
        "\n",
        "metric_results=furrr::future_map2_dbl(.x=grid_data$mtry,\n",
        "                                      .y=grid_data$trees,\n",
        "                                      ~xgb_tuning(c(.x, .y,0.5,10,0.01,0.2,0.8), validation_data))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfWzJ5MQgxUP",
        "colab_type": "text"
      },
      "source": [
        "Three - D plot of mtry,trees and AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K9t_lCgyE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "library(plotly)\n",
        "\n",
        "mtcars$am[which(mtcars$am == 0)] <- 'Automatic'\n",
        "mtcars$am[which(mtcars$am == 1)] <- 'Manual'\n",
        "mtcars$am <- as.factor(mtcars$am)\n",
        "\n",
        "p <- plot_ly(mtcars, x = ~wt, y = ~hp, z = ~qsec, color = ~am, colors = c('#BF382A', '#0C4B8E')) %>%\n",
        "  add_markers() %>%\n",
        "  layout(scene = list(xaxis = list(title = 'Weight'),\n",
        "                     yaxis = list(title = 'Gross horsepower'),\n",
        "                     zaxis = list(title = '1/4 mile time')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXLmlQJ8g4x8",
        "colab_type": "text"
      },
      "source": [
        "Multi-paramter Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lpjmX4ig5gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R\n",
        "\n",
        "\n",
        "df=tibble(\n",
        "  mtry=4:5,\n",
        "  trees=200:201,\n",
        "  min_n=c(0.5,0.6),\n",
        "  tree_depth=9:10,\n",
        "  learn_rate=c(0.01,0.02),\n",
        "  loss_reduction=c(0.2,0.3),\n",
        "  sample_size = c(0.7,0.8)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "xgb_tuning2 <- function(mtry,trees,min_n,tree_depth,learn_rate,loss_reduction,sample_size){\n",
        "    validation_data <-  validation_data\n",
        " \n",
        "\n",
        "    results <- purrr::map2_df(.x = validation_data$splits,\n",
        "                       .y = validation_data$id,\n",
        "                       ~my_xgb(mtry = mtry, trees = trees, \n",
        "                               min_n=min_n,tree_depth=tree_depth,\n",
        "                               learn_rate=learn_rate, \n",
        "                       loss_reduction=loss_reduction,\n",
        "                       sample_size=sample_size,\n",
        "                               \n",
        "                               split = .x, id = .y))\n",
        "\n",
        "  f_value <-  results %>%\n",
        "        group_by(id) %>%\n",
        "        f_meas(truth, prediction) %>%\n",
        "        summarise(mean_f_meas = mean(.estimate)) %>%\n",
        "        pull\n",
        "    \n",
        "    return(tibble(f_value=f_value))\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "metric_results=furrr::future_pmap_dfr(df, xgb_tuning2)\n",
        "\n",
        "\n",
        "#metric_results= future.apply::future_mapply(xgb_tuning2, \n",
        "#df$mtry,df$trees,df$min_n,df$tree_depth,df$learn_rate,df$loss_reduction,df$sample_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}